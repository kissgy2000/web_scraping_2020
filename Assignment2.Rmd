  
---
title: 'Assignment 2 (Web scrapig in R)'
author: Kiss Gyula
date: 2020-12-13
output:
  prettydoc::html_pretty:
    highlight: github
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
#install.packages("readtext")
library(readtext)
library(ggplot2)
library(stringr)
library(tidyr)
library(rvest)
library(data.table)

```

I use ft.com website to collect news for a search term, and do sentiment analysis.

I create a functions to download information for a keyword to dataframe, including article teaser and date of the article.

```{r, echo=TRUE, warning=FALSE}
my_url <- 'https://www.ft.com/search?q=hungary&page=4&sort=relevance'
get_data <- function(my_url){
    print(my_url)
    t <- read_html(my_url)
    boxes <- 
      t %>% 
      html_nodes('.o-teaser__standfirst')  
    boxes2 <- 
      t %>% 
      html_nodes('.o-teaser__timestamp')  
    
    x <- boxes[[1]]
    boxes_dfs <- lapply(boxes, function(x){
       tl <- list()
       tl[['title']] <- paste0(x %>% html_text(), collapse = ' ')
      return(tl)
    })
    boxes_dfs2 <- lapply(boxes2, function(x){
       tl <- list()
       tl[['date']] <- as.Date(paste0(x %>% html_text(), collapse = ' '), "%B %d, %Y")
      return(tl)
    })
    df <- rbindlist(boxes_dfs)
    df2 <- rbindlist(boxes_dfs2)
    
    return(cbind(df, df2))
}

```

```{r, echo=FALSE, warning=FALSE}
#df <- get_data(my_url)

```



```{r, echo=TRUE, warning=FALSE}
get_pages <- function(keyword, num_of_pages) {
  links <- 
    paste0('https://www.ft.com/search?q=', keyword, '&page=', 1:num_of_pages, '&sort=relevance' )
  ret_df <- rbindlist(lapply(links, get_data))
  return(ret_df)
  #https://www.ft.com/search?q=hungary&page=1&sort=relevance
}
```

Here I download articles for Hungary. The site does not allow more that 1000 articles to search for, so it is enough to download 40 pages.
```{r, echo=TRUE, warning=FALSE}
df <- get_pages('hungary', 40)
```
I do the following cleaning steps:

- eliminate non alpha characters

- tokenize

- eliminate some word, which are indifferent now

- eliminate stop words



```{r warning=FALSE}
tidy_df <- df  %>% unnest_tokens(word, title) %>% filter(grepl("(^[a-z]+).*", word))   %>%   
  anti_join(stop_words, by="word") %>%    
  filter (!(word %in% c("hungary", "orban", "viktor") ))
```

I created word frequency diagram.

```{r, echo=TRUE, warning=FALSE}
#knitr::kable(df)
tidy_df %>%
  count(word, sort = TRUE) %>%
  filter(n > 55) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

Now we check the sentiment by date based on the Bing sentiment dictionary.

```{r warning=FALSE}
df_sentiment <- tidy_df %>%
    inner_join(get_sentiments("bing"), by="word") %>%
    count(date, sentiment) %>%
    spread(sentiment, n, fill = 0) %>%
    mutate(sentiment = positive - negative)  

ggplot(df_sentiment, aes(date, sentiment, fill = sentiment)) +
  geom_col(show.legend = FALSE) 
```

We cannot say any conclusion based on this chart, maybe this dictionary is not appropriate for this kind of text.

Now we try another sentiment dictionary, with sentment index for the words, "afinn" dictionary.

```{r warning=FALSE}
df_sentiment2 <- tidy_df %>%
    inner_join(get_sentiments("afinn"),  by="word" ) %>% 
    group_by( date) %>% 
    summarise(sentiment = sum(value)) %>% 
    mutate(method = "AFINN")  

ggplot(df_sentiment2, aes(date, sentiment, fill = sentiment)) +
  theme (axis.text.x=element_text(angle=90)) +
  geom_col(show.legend = FALSE) #+ scale_x_continuous("year", labels = as.character(year))


```

We see that HUngary appears with negative sentiment in the last months.
